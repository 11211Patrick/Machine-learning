{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## HW3 Image Classification\n#### Solve image classification with convolutional neural networks(CNN).\n#### If you have any questions, please contact the TAs via TA hours, NTU COOL, or email to mlta-2023-spring@googlegroups.com","metadata":{}},{"cell_type":"code","source":"# check GPU type.\n!nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import Packages","metadata":{}},{"cell_type":"code","source":"_exp_name = \"sample\"","metadata":{"execution":{"iopub.status.busy":"2023-03-09T07:42:17.243055Z","iopub.execute_input":"2023-03-09T07:42:17.243803Z","iopub.status.idle":"2023-03-09T07:42:17.248875Z","shell.execute_reply.started":"2023-03-09T07:42:17.243765Z","shell.execute_reply":"2023-03-09T07:42:17.247738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import necessary packages.\nimport numpy as np\nimport pandas as pd\nimport torch\nimport os\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import models\nimport torchvision.transforms as transforms\nfrom PIL import Image\n# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\nfrom torch.utils.data import Dataset, DataLoader, Subset, TensorDataset, random_split, SubsetRandomSampler, ConcatDataset\nfrom torchvision.datasets import DatasetFolder, VisionDataset\n# This is for the progress bar.\nfrom tqdm.auto import tqdm\nimport random","metadata":{"execution":{"iopub.status.busy":"2023-03-09T07:42:11.819489Z","iopub.execute_input":"2023-03-09T07:42:11.820324Z","iopub.status.idle":"2023-03-09T07:42:14.255532Z","shell.execute_reply.started":"2023-03-09T07:42:11.820281Z","shell.execute_reply":"2023-03-09T07:42:14.254504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"myseed = 1213  # set a random seed for reproducibility\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(myseed)\ntorch.manual_seed(myseed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(myseed)","metadata":{"execution":{"iopub.status.busy":"2023-03-09T07:42:19.977151Z","iopub.execute_input":"2023-03-09T07:42:19.978066Z","iopub.status.idle":"2023-03-09T07:42:20.047735Z","shell.execute_reply.started":"2023-03-09T07:42:19.978012Z","shell.execute_reply":"2023-03-09T07:42:20.04666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transforms","metadata":{}},{"cell_type":"code","source":"# Normally, We don't need augmentations in testing and validation.\n# All we need here is to resize the PIL image and transform it into Tensor.\ntest_tfm = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n])\n\n# Posibility of transforms\ntfm_p = 0.3\n\n# However, it is also possible to use augmentation in the testing phase.\n# You may use train_tfm to produce a variety of images and then test using ensemble methods\ntrain_tfm = transforms.Compose([\n    # Resize the image into a fixed shape (height = width = 128)\n    transforms.Resize((128, 128)),\n    # You may add some transforms here.\n    transforms.RandomAutocontrast(p=tfm_p),\n    transforms.RandomEqualize(p=tfm_p),\n    transforms.RandomPerspective(p=tfm_p),\n    transforms.RandomVerticalFlip(p=tfm_p),\n    transforms.RandomHorizontalFlip(p=tfm_p),\n    # ToTensor() should be the last one of the transforms.\n    transforms.ToTensor(),\n])","metadata":{"execution":{"iopub.status.busy":"2023-03-09T07:42:21.552569Z","iopub.execute_input":"2023-03-09T07:42:21.552959Z","iopub.status.idle":"2023-03-09T07:42:21.561151Z","shell.execute_reply.started":"2023-03-09T07:42:21.552921Z","shell.execute_reply":"2023-03-09T07:42:21.557799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Datasets","metadata":{}},{"cell_type":"code","source":"class FoodDataset(Dataset):\n\n    def __init__(self,path,tfm=test_tfm,files = None):\n        super(FoodDataset).__init__()\n        self.path = path\n        self.files = sorted([os.path.join(path,x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n        if files != None:\n            self.files = files\n            \n        self.transform = tfm\n  \n    def __len__(self):\n        return len(self.files)\n  \n    def __getitem__(self,idx):\n        fname = self.files[idx]\n        im = Image.open(fname)\n        im = self.transform(im)\n        \n        try:\n            label = int(fname.split(\"/\")[-1].split(\"_\")[0])\n        except:\n            label = -1 # test has no label\n            \n        return im,label","metadata":{"execution":{"iopub.status.busy":"2023-03-09T07:42:24.685824Z","iopub.execute_input":"2023-03-09T07:42:24.686187Z","iopub.status.idle":"2023-03-09T07:42:24.694887Z","shell.execute_reply.started":"2023-03-09T07:42:24.686154Z","shell.execute_reply":"2023-03-09T07:42:24.693823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"# Posibility of dropout layers\ndropout_layer_p = 0.2\n\nclass Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n        # input 維度 [3, 128, 128]\n        self.cnn = nn.Sequential(\n            nn.Conv2d(3, 64, 3, 1, 1),  # [64, 128, 128]\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2, 0),      # [64, 64, 64]\n\n            nn.Conv2d(64, 128, 3, 1, 1), # [128, 64, 64]\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2, 0),      # [128, 32, 32]\n            \n            \n            nn.Conv2d(128, 256, 3, 1, 1), # [256, 32, 32]\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2, 0),      # [256, 16, 16]\n            \n            nn.Conv2d(256, 384, 3, 1, 1), # [384, 16, 16]\n            nn.BatchNorm2d(384),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2, 0),      # [384, 8, 8]\n            \n            nn.Conv2d(384, 512, 3, 1, 1), # [512, 8, 8]\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.Dropout2d(p=dropout_layer_p),\n        \n            nn.Conv2d(512, 640, 3, 1, 1), # [640, 8, 8]\n            nn.BatchNorm2d(640),\n            nn.ReLU(),\n            nn.Dropout2d(p=dropout_layer_p),\n            \n            nn.Conv2d(640, 768, 3, 1, 1), # [768, 8, 8]\n            nn.BatchNorm2d(768),\n            nn.ReLU(),\n            nn.Dropout2d(p=dropout_layer_p),\n            \n            nn.Conv2d(768, 896, 3, 1, 1), # [768, 8, 8]\n            nn.BatchNorm2d(896),\n            nn.ReLU(),\n            nn.Dropout2d(p=dropout_layer_p),\n            \n            \n            nn.Conv2d(896, 896, 3, 1, 1), # [640, 8, 8]\n            nn.BatchNorm2d(896),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2, 0),       # [640, 4, 4]\n            nn.Dropout2d(p=dropout_layer_p)\n        )\n        self.fc = nn.Sequential(\n            nn.Linear(896*4*4, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 11)\n        )\n\n    def forward(self, x):\n        \n        out = self.cnn(x)\n        \n        out = out.view(out.size()[0], -1)\n        return self.fc(out)","metadata":{"execution":{"iopub.status.busy":"2023-03-09T07:42:27.227238Z","iopub.execute_input":"2023-03-09T07:42:27.227927Z","iopub.status.idle":"2023-03-09T07:42:27.240562Z","shell.execute_reply.started":"2023-03-09T07:42:27.227883Z","shell.execute_reply":"2023-03-09T07:42:27.239131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Configurations","metadata":{}},{"cell_type":"code","source":"# \"cuda\" only when GPUs are available.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Times of cross Validation\ncross = 2\n\n# The number of training epochs.\nn_epochs = 150\n\n# Initialize a model, and put it on the device specified.\n# model = Classifier().to(device)\nmodel = []\noptimizer = []\nfor i in range(cross):\n    model.append(Classifier().to(device))\n    # num_ftrs = model[i].fc.in_features\n    # model[i].fc = nn.Linear(num_ftrs, 11)\n    # model[i].to(device)\n    # Initialize optimizer, you may fine-tune some hyperparameters such as learning rate on your own.\n    optimizer.append(torch.optim.Adam(model[i].parameters(), lr=1e-3, weight_decay=1e-4, amsgrad=True))\n\n    \n# The number of batch size.\nbatch_size = 256\n\n\n# If no improvement in 'patience' epochs, early stop.\npatience = 40\n\n# The ratio of test_time_augmentation\ntest_time_augmentation = 0.8\n\n# For the classification task, we use cross-entropy as the measurement of performance.\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2023-03-09T07:46:47.006132Z","iopub.execute_input":"2023-03-09T07:46:47.006501Z","iopub.status.idle":"2023-03-09T07:46:47.219777Z","shell.execute_reply.started":"2023-03-09T07:46:47.006464Z","shell.execute_reply":"2023-03-09T07:46:47.218667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataloader","metadata":{}},{"cell_type":"code","source":"# Construct train and valid datasets.\n# The argument \"loader\" tells how torchvision reads the data.\ntrain_set = FoodDataset(\"/kaggle/input/ml2023spring-hw3/train\", tfm=train_tfm)\n#train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\nvalid_set = FoodDataset(\"/kaggle/input/ml2023spring-hw3/valid\", tfm=test_tfm)\n#valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n\ndataset = ConcatDataset([train_set, valid_set])\n\n# print(\"nananananasasasasawawawawamimimimimiaaaaa\")\ntrain_size = int(0.733 * len(dataset))\ntest_size = len(dataset) - train_size\n","metadata":{"execution":{"iopub.status.busy":"2023-03-09T07:42:54.652962Z","iopub.execute_input":"2023-03-09T07:42:54.653791Z","iopub.status.idle":"2023-03-09T07:42:54.694928Z","shell.execute_reply.started":"2023-03-09T07:42:54.653747Z","shell.execute_reply":"2023-03-09T07:42:54.69389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Start Training","metadata":{}},{"cell_type":"code","source":"###### Learning rate schedule\n'''def adjust_learning_rate(optimizer, epoch):\n    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n    modellrnew = modellr * (0.99 ** epoch)\n    print(\"lr:\", modellrnew)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = modellrnew'''\n\n# Initialize trackers, these are not parameters and should not be changed\nstale = 0\n\nfor i in range(cross):\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n    \n    best_acc = 0\n    \n    ###### Randomly splitting the set\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n    valid_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n    \n    for epoch in range(n_epochs):\n\n        # ---------- Training ----------\n        # Make sure the model is in train mode before training.\n        model[i].train()\n\n        # These are used to record information in training.\n        train_loss = []\n        train_accs = []\n\n        for batch in tqdm(train_loader):\n\n            # A batch consists of image data and corresponding labels.\n            imgs, labels = batch\n            #imgs = imgs.half()\n            #print(imgs.shape,labels.shape)\n\n            # Forward the data. (Make sure data and model are on the same device.)\n            logits = model[i](imgs.to(device))\n\n            # Calculate the cross-entropy loss.\n            # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n            loss = criterion(logits, labels.to(device))\n\n            # Gradients stored in the parameters in the previous step should be cleared out first.\n            optimizer[i].zero_grad()\n\n            # Compute the gradients for parameters.\n            loss.backward()\n\n            # Clip the gradient norms for stable training.\n            grad_norm = nn.utils.clip_grad_norm_(model[i].parameters(), max_norm=10)\n\n            # Update the parameters with computed gradients.\n            optimizer[i].step()\n\n            # Compute the accuracy for current batch.\n            acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n\n            # Record the loss and accuracy.\n            train_loss.append(loss.item())\n            train_accs.append(acc)\n\n        train_loss = sum(train_loss) / len(train_loss)\n        train_acc = sum(train_accs) / len(train_accs)\n\n        # Print the information.\n        print(f\"[ Train | No.{i+1} loop | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n\n        # ---------- Validation ----------\n        # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n        model[i].eval()\n\n        # These are used to record information in validation.\n        valid_loss = []\n        valid_accs = []\n\n        # Iterate the validation set by batches.\n        for batch in tqdm(valid_loader):\n\n            # A batch consists of image data and corresponding labels.\n            imgs, labels = batch\n            #imgs = imgs.half()\n\n            # We don't need gradient in validation.\n            # Using torch.no_grad() accelerates the forward process.\n            with torch.no_grad():\n                logits = model[i](imgs.to(device))\n\n            # We can still compute the loss (but not the gradient).\n            loss = criterion(logits, labels.to(device))\n\n            # Compute the accuracy for current batch.\n            acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n\n            # Record the loss and accuracy.\n            valid_loss.append(loss.item())\n            valid_accs.append(acc)\n            #break\n\n        # The average loss and accuracy for entire validation set is the average of the recorded values.\n        valid_loss = sum(valid_loss) / len(valid_loss)\n        valid_acc = sum(valid_accs) / len(valid_accs)\n\n        # Print the information.\n        print(f\"[ Valid | No.{i+1} loop | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n\n\n        # update logs\n        if valid_acc > best_acc:\n            with open(f\"./{_exp_name}_log.txt\",\"a\"):\n                print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f} -> best\")\n        else:\n            with open(f\"./{_exp_name}_log.txt\",\"a\"):\n                print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n\n\n        # save models\n        if valid_acc > best_acc:\n            print(f\"Best model {i+1} found at epoch {epoch+1}, saving model\")\n            torch.save(model[i].state_dict(), f\"{_exp_name}_best{i}.ckpt\") # only save best to prevent output memory exceed error\n            best_acc = valid_acc\n            stale = 0\n        else:\n            stale += 1\n            if stale > patience:\n                print(f\"No improvment {patience} consecutive epochs, early stopping\")\n                break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataloader for test","metadata":{}},{"cell_type":"code","source":"# Construct test datasets.\n# The argument \"loader\" tells how torchvision reads the data.\ntest_set = FoodDataset(\"/kaggle/input/ml2023spring-hw3/test\", tfm=test_tfm)\ntest_set1 = FoodDataset(\"/kaggle/input/ml2023spring-hw3/test\", tfm=train_tfm)\ntest_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\ntest_loader1 = DataLoader(test_set1, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-09T07:46:54.198074Z","iopub.execute_input":"2023-03-09T07:46:54.198454Z","iopub.status.idle":"2023-03-09T07:46:55.006405Z","shell.execute_reply.started":"2023-03-09T07:46:54.198422Z","shell.execute_reply":"2023-03-09T07:46:55.005309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing and generate prediction CSV","metadata":{}},{"cell_type":"code","source":"model_best = []\nfor i in range(cross):\n    model_best.append(Classifier().to(device))\n    #num_ftrs = model_best[i].fc.in_features\n    #model_best[i].fc = nn.Linear(num_ftrs, 11)\n    #model_best[i].to(device)\n    model_best[i].load_state_dict(torch.load(f\"{_exp_name}_best{i}.ckpt\"))\n    model_best[i].eval()\n\nprediction = []\nprediction0 = []\nprediction1 = []\n\nwith torch.no_grad():\n    \n    for k in range(cross):\n        i = 0\n        for data,_ in tqdm(test_loader):\n            test_pred = model_best[k](data.to(device))\n            if k != 0:\n                prediction0[i] = np.add( prediction0[i] , test_pred.cpu().data.numpy())\n                i += 1\n            else:\n                prediction0.append(test_pred.cpu().data.numpy())\n            #test_label = test_time_augmentation * np.argmax(test_pred.cpu().data.numpy(), axis=1)\n            #prediction += test_label.squeeze().tolist()\n    prediction0 = [ i / cross for i in prediction0 ]\n        \nwith torch.no_grad():\n    for k in range(cross):\n        i = 0\n        for data,_ in tqdm(test_loader1):\n            test_pred = model_best[k](data.to(device))\n            if k != 0:\n                prediction1[i] = np.add( prediction1[i] , test_pred.cpu().data.numpy())\n                i += 1\n            else:\n                prediction1.append(test_pred.cpu().data.numpy())\n            \n    prediction1 = [ i / cross for i in prediction1 ]\n    \n \n        #test_label = (1 - test_time_augmentation) * np.argmax(test_pred.cpu().data.numpy(), axis=1)\n        #prediction1 += test_label.squeeze().tolist()\n\nfor i in range(len(prediction0)):\n    prediction0[i] = np.add(test_time_augmentation * prediction0[i],(1-test_time_augmentation) * prediction1[i])\n    test_label = np.argmax(prediction0[i], axis=1)\n    prediction += test_label.squeeze().tolist()\n\n###### Test time augmentation\n# prediction = np.around(np.sum([prediction,prediction1],axis=0).tolist()).astype(int)\n\n#print(len(prediction))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create test csv\ndef pad4(i):\n    return \"0\"*(4-len(str(i)))+str(i)\ndf = pd.DataFrame()\ndf[\"Id\"] = [pad4(i) for i in range(len(test_set))]\ndf[\"Category\"] = prediction\ndf.to_csv(\"submission.csv\",index = False)","metadata":{"execution":{"iopub.status.busy":"2023-03-08T13:32:54.266019Z","iopub.execute_input":"2023-03-08T13:32:54.267075Z","iopub.status.idle":"2023-03-08T13:32:54.297109Z","shell.execute_reply.started":"2023-03-08T13:32:54.267021Z","shell.execute_reply":"2023-03-08T13:32:54.296149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Q1. Augmentation Implementation\n### Implement augmentation by finishing train_tfm in the code with image size of your choice. \n### Directly copy the following block and paste it on GradeScope after you finish the code\n#### Your train_tfm must be capable of producing 5+ different results when given an identical image multiple times.\n#### Your  train_tfm in the report can be different from train_tfm in your training code.","metadata":{}},{"cell_type":"code","source":"train_tfm = transforms.Compose([\n    # Resize the image into a fixed shape (height = width = 128)\n    transforms.Resize((128, 128)),\n    # You can add some transforms here.\n    #\n    tfm_p = 0.1,\n    # Adjust the sharpness of the image randomly with a given probability.\n    # 0 gives a blurred image. 1,2是增加敏銳度，我選讓圖片模糊是覺得這樣跟原圖會比較不同\n    transforms.RandomAdjustSharpness(sharpness_factor=0, p=tfm_p),\n    \n    # Equalize the histogram of the given image randomly with a given probability.\n    transforms.RandomEqualize(p=tfm_p),\n    \n    #會讓圖片看起來某一邊離比較近，一邊比較遠(會呈現梯形)\n    transforms.RandomPerspective(p=tfm_p),\n    \n    #會讓圖片上下顛倒\n    transforms.RandomVerticalFlip(p=tfm_p),\n    \n    #會讓圖片左右顛倒\n    transforms.RandomHorizontalFlip(p=tfm_p),\n    \n    transforms.ToTensor(),\n])","metadata":{"execution":{"iopub.status.busy":"2023-03-09T07:47:13.627855Z","iopub.execute_input":"2023-03-09T07:47:13.628563Z","iopub.status.idle":"2023-03-09T07:47:13.634188Z","shell.execute_reply.started":"2023-03-09T07:47:13.628524Z","shell.execute_reply":"2023-03-09T07:47:13.632826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Q2. Visual Representations Implementation\n### Visualize the learned visual representations of the CNN model on the validation set by implementing t-SNE (t-distributed Stochastic Neighbor Embedding) on the output of both top & mid layers (You need to submit 2 images). \n### ChatGPT has generated the following code, which requires a minor modification to produce the expected results.","metadata":{}},{"cell_type":"code","source":"'''import torch\nimport numpy as np\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport matplotlib.cm as cm\nimport torch.nn as nn\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Load the trained model\nmodel = Classifier().to(device)\nstate_dict = torch.load(f\"{_exp_name}_best.ckpt\")\nmodel.load_state_dict(state_dict)\nmodel.eval()\n\nprint(model)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''# Load the vaildation set defined by TA\nvalid_set = FoodDataset(\"/kaggle/input/ml2023spring-hw3/valid\", tfm=test_tfm)\nvalid_loader = DataLoader(valid_set, batch_size=64, shuffle=False, num_workers=0, pin_memory=True)\n\n# Extract the representations for the specific layer of model\nindex = 14 # You should find out the index of layer which is defined as \"top\" or 'mid' layer of your model.\nfeatures = []\nlabels = []\nfor batch in tqdm(valid_loader):\n    imgs, lbls = batch\n    with torch.no_grad():\n        logits = model.cnn[:index](imgs.to(device))\n        logits = logits.view(logits.size()[0], -1)\n    labels.extend(lbls.cpu().numpy())\n    logits = np.squeeze(logits.cpu().numpy())\n    features.extend(logits)\n    \nfeatures = np.array(features)\ncolors_per_class = cm.rainbow(np.linspace(0, 1, 11))\n\n# Apply t-SNE to the features\nfeatures_tsne = TSNE(n_components=2, init='pca', random_state=42).fit_transform(features)\n\n# Plot the t-SNE visualization\nplt.figure(figsize=(10, 8))\nfor label in np.unique(labels):\n    plt.scatter(features_tsne[labels == label, 0], features_tsne[labels == label, 1], label=label, s=5)\nplt.legend()\nplt.show()'''","metadata":{},"execution_count":null,"outputs":[]}]}
